nohup: ignoring input
2024-06-26:16:00:15,011 INFO     [__main__.py:223] Verbosity set to INFO
2024-06-26:16:00:15,012 INFO     [__init__.py:369] lm_eval.tasks.initialize_tasks() is deprecated and no longer necessary. It will be removed in v0.4.2 release. TaskManager will instead be used.
2024-06-26:16:00:19,090 INFO     [__main__.py:307] Selected Tasks: ['gsm8k']
2024-06-26:16:00:19,090 INFO     [__main__.py:308] Loading selected tasks...
2024-06-26:16:00:19,091 INFO     [evaluator.py:135] Setting random seed to 0 | Setting numpy seed to 1234 | Setting torch manual seed to 1234
2024-06-26:16:00:19,171 WARNING  [other.py:349] Detected kernel version 5.4.0, which is below the recommended minimum of 5.5.0; this can cause the process to hang. It is recommended to upgrade the kernel to the minimum version or higher.
2024-06-26:16:00:19,171 INFO     [llama_qt.py:163] Using device 'cuda'
The argument `trust_remote_code` is to be used with Auto classes. It has no effect here and is ignored.
Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]Loading checkpoint shards:  50%|█████     | 1/2 [00:49<00:49, 49.02s/it]Loading checkpoint shards: 100%|██████████| 2/2 [01:05<00:00, 30.05s/it]Loading checkpoint shards: 100%|██████████| 2/2 [01:05<00:00, 32.90s/it]
2024-06-26:16:01:47,217 INFO     [evaluator.py:193] get_task_dict has been updated to accept an optional argument, `task_manager`Read more here:https://github.com/EleutherAI/lm-evaluation-harness/blob/main/docs/interface.md#external-library-usage
2024-06-26:16:01:49,482 WARNING  [evaluator.py:225] Overwriting default num_fewshot of gsm8k from 5 to 5
2024-06-26:16:01:49,483 INFO     [task.py:386] Building contexts for gsm8k on rank 0...
  0%|          | 0/1319 [00:00<?, ?it/s]  2%|▏         | 22/1319 [00:00<00:06, 211.06it/s]  3%|▎         | 44/1319 [00:00<00:06, 210.52it/s]  5%|▌         | 66/1319 [00:00<00:05, 211.36it/s]  7%|▋         | 88/1319 [00:00<00:05, 212.46it/s]  8%|▊         | 110/1319 [00:00<00:05, 212.76it/s] 10%|█         | 132/1319 [00:00<00:05, 213.13it/s] 12%|█▏        | 154/1319 [00:00<00:05, 213.61it/s] 13%|█▎        | 176/1319 [00:00<00:05, 213.67it/s] 15%|█▌        | 198/1319 [00:00<00:05, 214.55it/s] 17%|█▋        | 220/1319 [00:01<00:05, 215.35it/s] 18%|█▊        | 242/1319 [00:01<00:05, 212.85it/s] 20%|██        | 264/1319 [00:01<00:04, 212.85it/s] 22%|██▏       | 286/1319 [00:01<00:04, 212.74it/s] 23%|██▎       | 308/1319 [00:01<00:04, 212.88it/s] 25%|██▌       | 330/1319 [00:01<00:04, 213.43it/s] 27%|██▋       | 352/1319 [00:01<00:04, 214.01it/s] 28%|██▊       | 374/1319 [00:01<00:04, 211.04it/s] 30%|███       | 396/1319 [00:01<00:04, 205.44it/s] 32%|███▏      | 418/1319 [00:01<00:04, 207.65it/s] 33%|███▎      | 440/1319 [00:02<00:04, 209.98it/s] 35%|███▌      | 462/1319 [00:02<00:04, 211.52it/s] 37%|███▋      | 484/1319 [00:02<00:03, 212.80it/s] 38%|███▊      | 506/1319 [00:02<00:03, 213.62it/s] 40%|████      | 528/1319 [00:02<00:03, 214.11it/s] 42%|████▏     | 550/1319 [00:02<00:03, 214.44it/s] 43%|████▎     | 572/1319 [00:02<00:03, 214.77it/s] 45%|████▌     | 594/1319 [00:02<00:03, 214.67it/s] 47%|████▋     | 616/1319 [00:02<00:03, 214.65it/s] 48%|████▊     | 638/1319 [00:02<00:03, 214.93it/s] 50%|█████     | 660/1319 [00:03<00:03, 215.02it/s] 52%|█████▏    | 682/1319 [00:03<00:02, 214.96it/s] 53%|█████▎    | 704/1319 [00:03<00:02, 214.90it/s] 55%|█████▌    | 726/1319 [00:03<00:02, 214.94it/s] 57%|█████▋    | 748/1319 [00:03<00:02, 214.59it/s] 58%|█████▊    | 770/1319 [00:03<00:02, 214.82it/s] 60%|██████    | 792/1319 [00:03<00:02, 214.83it/s] 62%|██████▏   | 814/1319 [00:03<00:02, 214.76it/s] 63%|██████▎   | 836/1319 [00:03<00:02, 214.55it/s] 65%|██████▌   | 858/1319 [00:04<00:02, 214.05it/s] 67%|██████▋   | 880/1319 [00:04<00:02, 214.61it/s] 68%|██████▊   | 902/1319 [00:04<00:01, 214.57it/s] 70%|███████   | 924/1319 [00:04<00:01, 214.76it/s] 72%|███████▏  | 946/1319 [00:04<00:01, 215.07it/s] 73%|███████▎  | 968/1319 [00:04<00:01, 212.08it/s] 75%|███████▌  | 990/1319 [00:04<00:01, 212.63it/s] 77%|███████▋  | 1012/1319 [00:04<00:01, 213.06it/s] 78%|███████▊  | 1034/1319 [00:04<00:01, 213.34it/s] 80%|████████  | 1056/1319 [00:04<00:01, 213.98it/s] 82%|████████▏ | 1078/1319 [00:05<00:01, 214.34it/s] 83%|████████▎ | 1100/1319 [00:05<00:01, 214.69it/s] 85%|████████▌ | 1122/1319 [00:05<00:00, 214.91it/s] 87%|████████▋ | 1144/1319 [00:05<00:00, 214.82it/s] 88%|████████▊ | 1166/1319 [00:05<00:00, 214.86it/s] 90%|█████████ | 1188/1319 [00:05<00:00, 214.71it/s] 92%|█████████▏| 1210/1319 [00:05<00:00, 214.75it/s] 93%|█████████▎| 1232/1319 [00:05<00:00, 214.90it/s] 95%|█████████▌| 1254/1319 [00:05<00:00, 213.71it/s] 97%|█████████▋| 1276/1319 [00:05<00:00, 214.14it/s] 98%|█████████▊| 1298/1319 [00:06<00:00, 214.71it/s]100%|██████████| 1319/1319 [00:06<00:00, 213.63it/s]
2024-06-26:16:01:55,678 INFO     [evaluator.py:365] Running generate_until requests
Running generate_until requests:   0%|          | 0/1319 [00:00<?, ?it/s]Passed argument batch_size = auto. Detecting largest batch size
Traceback (most recent call last):
  File "/scratch0/zx22/zijie/miniconda3/envs/llama/bin/lm_eval", line 8, in <module>
    sys.exit(cli_evaluate())
  File "/scratch1/zx22/zijie/lm-evaluation-harness/lm_eval/__main__.py", line 314, in cli_evaluate
    results = evaluator.simple_evaluate(
  File "/scratch1/zx22/zijie/lm-evaluation-harness/lm_eval/utils.py", line 288, in _wrapper
    return fn(*args, **kwargs)
  File "/scratch1/zx22/zijie/lm-evaluation-harness/lm_eval/evaluator.py", line 233, in simple_evaluate
    results = evaluate(
  File "/scratch1/zx22/zijie/lm-evaluation-harness/lm_eval/utils.py", line 288, in _wrapper
    return fn(*args, **kwargs)
  File "/scratch1/zx22/zijie/lm-evaluation-harness/lm_eval/evaluator.py", line 376, in evaluate
    resps = getattr(lm, reqtype)(cloned_reqs)
  File "/scratch1/zx22/zijie/lm-evaluation-harness/lm_eval/models/llama_qt.py", line 1127, in generate_until
    batch_size = self._detect_batch_size()
  File "/scratch1/zx22/zijie/lm-evaluation-harness/lm_eval/models/llama_qt.py", line 665, in _detect_batch_size
    batch_size = forward_batch()
  File "/scratch0/zx22/zijie/miniconda3/envs/llama/lib/python3.10/site-packages/accelerate/utils/memory.py", line 142, in decorator
    return function(batch_size, *args, **kwargs)
  File "/scratch1/zx22/zijie/lm-evaluation-harness/lm_eval/models/llama_qt.py", line 660, in forward_batch
    out = F.log_softmax(self._model_call(test_batch, **call_kwargs), dim=-1)  # noqa: F841
  File "/scratch1/zx22/zijie/lm-evaluation-harness/lm_eval/models/llama_qt.py", line 766, in _model_call
    return self.model(inps).logits
  File "/scratch0/zx22/zijie/miniconda3/envs/llama/lib/python3.10/site-packages/torch/nn/modules/module.py", line 1518, in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
  File "/scratch0/zx22/zijie/miniconda3/envs/llama/lib/python3.10/site-packages/torch/nn/modules/module.py", line 1527, in _call_impl
    return forward_call(*args, **kwargs)
  File "/scratch1/zx22/zijie/lm-evaluation-harness/lm_eval/models/llama_qt_utils.py", line 752, in forward
    outputs = self.model(
  File "/scratch0/zx22/zijie/miniconda3/envs/llama/lib/python3.10/site-packages/torch/nn/modules/module.py", line 1518, in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
  File "/scratch0/zx22/zijie/miniconda3/envs/llama/lib/python3.10/site-packages/torch/nn/modules/module.py", line 1527, in _call_impl
    return forward_call(*args, **kwargs)
  File "/scratch1/zx22/zijie/lm-evaluation-harness/lm_eval/models/llama_qt_utils.py", line 551, in forward
    layer_outputs = decoder_layer(
  File "/scratch0/zx22/zijie/miniconda3/envs/llama/lib/python3.10/site-packages/torch/nn/modules/module.py", line 1518, in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
  File "/scratch0/zx22/zijie/miniconda3/envs/llama/lib/python3.10/site-packages/torch/nn/modules/module.py", line 1527, in _call_impl
    return forward_call(*args, **kwargs)
  File "/scratch1/zx22/zijie/lm-evaluation-harness/lm_eval/models/llama_qt_utils.py", line 291, in forward
    hidden_states, self_attn_weights, present_key_value = self.self_attn(
  File "/scratch0/zx22/zijie/miniconda3/envs/llama/lib/python3.10/site-packages/torch/nn/modules/module.py", line 1518, in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
  File "/scratch0/zx22/zijie/miniconda3/envs/llama/lib/python3.10/site-packages/torch/nn/modules/module.py", line 1527, in _call_impl
    return forward_call(*args, **kwargs)
  File "/scratch1/zx22/zijie/lm-evaluation-harness/lm_eval/models/llama_qt_utils.py", line 174, in forward
    quantized_key_states = dequantize_per_head(quantize_per_head(key_states_trans))
TypeError: quantize_per_head() missing 1 required positional argument: 'bit'
Running generate_until requests:   0%|          | 0/1319 [00:01<?, ?it/s]
