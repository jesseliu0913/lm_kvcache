nohup: ignoring input
2024-07-02:22:10:36,760 INFO     [__main__.py:223] Verbosity set to INFO
2024-07-02:22:10:36,760 INFO     [__init__.py:369] lm_eval.tasks.initialize_tasks() is deprecated and no longer necessary. It will be removed in v0.4.2 release. TaskManager will instead be used.
2024-07-02:22:10:40,886 INFO     [__main__.py:307] Selected Tasks: ['coqa']
2024-07-02:22:10:40,887 INFO     [__main__.py:308] Loading selected tasks...
2024-07-02:22:10:40,894 INFO     [evaluator.py:135] Setting random seed to 0 | Setting numpy seed to 1234 | Setting torch manual seed to 1234
2024-07-02:22:10:41,059 WARNING  [other.py:349] Detected kernel version 5.4.0, which is below the recommended minimum of 5.5.0; this can cause the process to hang. It is recommended to upgrade the kernel to the minimum version or higher.
2024-07-02:22:10:41,059 INFO     [llama_qt.py:167] Using device 'cuda'
The argument `trust_remote_code` is to be used with Auto classes. It has no effect here and is ignored.
Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]Loading checkpoint shards:  50%|█████     | 1/2 [00:03<00:03,  3.69s/it]Loading checkpoint shards: 100%|██████████| 2/2 [00:04<00:00,  2.18s/it]Loading checkpoint shards: 100%|██████████| 2/2 [00:04<00:00,  2.41s/it]
2024-07-02:22:10:46,567 INFO     [evaluator.py:193] get_task_dict has been updated to accept an optional argument, `task_manager`Read more here:https://github.com/EleutherAI/lm-evaluation-harness/blob/main/docs/interface.md#external-library-usage
2024-07-02:22:10:48,903 INFO     [task.py:386] Building contexts for coqa on rank 0...
  0%|          | 0/500 [00:00<?, ?it/s] 19%|█▉        | 97/500 [00:00<00:01, 352.63it/s]100%|██████████| 500/500 [00:00<00:00, 1772.81it/s]
2024-07-02:22:10:49,333 INFO     [evaluator.py:365] Running generate_until requests
Running generate_until requests:   0%|          | 0/500 [00:00<?, ?it/s]/scratch0/zx22/zijie/miniconda3/envs/llama/lib/python3.10/site-packages/transformers/generation/configuration_utils.py:540: UserWarning: `do_sample` is set to `False`. However, `temperature` is set to `0.6` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `temperature`.
  warnings.warn(
/scratch0/zx22/zijie/miniconda3/envs/llama/lib/python3.10/site-packages/transformers/generation/configuration_utils.py:545: UserWarning: `do_sample` is set to `False`. However, `top_p` is set to `0.9` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `top_p`.
  warnings.warn(
Running generate_until requests:   0%|          | 1/500 [00:34<4:49:22, 34.79s/it]Running generate_until requests:   1%|          | 3/500 [00:48<1:55:49, 13.98s/it]Running generate_until requests:   1%|          | 5/500 [01:01<1:24:16, 10.22s/it]Running generate_until requests:   1%|▏         | 7/500 [01:03<51:40,  6.29s/it]  Running generate_until requests:   2%|▏         | 9/500 [01:04<34:09,  4.17s/it]Running generate_until requests:   2%|▏         | 11/500 [01:17<40:52,  5.02s/it]Running generate_until requests:   3%|▎         | 13/500 [01:31<44:56,  5.54s/it]Running generate_until requests:   3%|▎         | 15/500 [01:44<47:26,  5.87s/it]Running generate_until requests:   3%|▎         | 17/500 [01:46<34:49,  4.33s/it]Running generate_until requests:   4%|▍         | 19/500 [01:47<25:11,  3.14s/it]Running generate_until requests:   4%|▍         | 21/500 [02:00<33:27,  4.19s/it]Running generate_until requests:   5%|▍         | 23/500 [02:13<39:05,  4.92s/it]Running generate_until requests:   5%|▌         | 25/500 [02:26<42:54,  5.42s/it]Running generate_until requests:   5%|▌         | 27/500 [02:27<31:29,  4.00s/it]