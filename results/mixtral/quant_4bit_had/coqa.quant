nohup: ignoring input
2024-07-03 10:06:34.017368: I tensorflow/core/platform/cpu_feature_guard.cc:182] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.
To enable the following instructions: AVX2 FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.
2024-07-03 10:06:34.965622: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT
[2024-07-03 10:06:37,265] [INFO] [real_accelerator.py:110:get_accelerator] Setting ds_accelerator to cuda (auto detect)
2024-07-03:10:06:38,674 INFO     [__main__.py:223] Verbosity set to INFO
2024-07-03:10:06:38,675 INFO     [__init__.py:369] lm_eval.tasks.initialize_tasks() is deprecated and no longer necessary. It will be removed in v0.4.2 release. TaskManager will instead be used.
2024-07-03:10:06:43,587 INFO     [__main__.py:307] Selected Tasks: ['coqa']
2024-07-03:10:06:43,587 INFO     [__main__.py:308] Loading selected tasks...
2024-07-03:10:06:43,588 INFO     [evaluator.py:135] Setting random seed to 0 | Setting numpy seed to 1234 | Setting torch manual seed to 1234
2024-07-03:10:06:43,594 INFO     [mistral_qt.py:166] Using device 'cuda'
The argument `trust_remote_code` is to be used with Auto classes. It has no effect here and is ignored.
Loading checkpoint shards:   0%|          | 0/3 [00:00<?, ?it/s]Loading checkpoint shards:  33%|███▎      | 1/3 [00:05<00:10,  5.46s/it]Loading checkpoint shards:  67%|██████▋   | 2/3 [00:08<00:04,  4.16s/it]Loading checkpoint shards:  67%|██████▋   | 2/3 [00:10<00:05,  5.24s/it]
Traceback (most recent call last):
  File "/home/zx22/.conda/envs/llama/bin/lm_eval", line 8, in <module>
    sys.exit(cli_evaluate())
  File "/scratch/zx22/zijie/lm_kvcache/lm_eval/__main__.py", line 314, in cli_evaluate
    results = evaluator.simple_evaluate(
  File "/scratch/zx22/zijie/lm_kvcache/lm_eval/utils.py", line 288, in _wrapper
    return fn(*args, **kwargs)
  File "/scratch/zx22/zijie/lm_kvcache/lm_eval/evaluator.py", line 166, in simple_evaluate
    lm = lm_eval.api.registry.get_model(model).create_from_arg_string(
  File "/scratch/zx22/zijie/lm_kvcache/lm_eval/api/model.py", line 134, in create_from_arg_string
    return cls(**args, **args2)
  File "/scratch/zx22/zijie/lm_kvcache/lm_eval/models/mistral_qt.py", line 205, in __init__
    self._create_model(
  File "/scratch/zx22/zijie/lm_kvcache/lm_eval/models/mistral_qt.py", line 564, in _create_model
    self._model = CustomedMistralForCausalLM.from_pretrained(
  File "/home/zx22/.conda/envs/llama/lib/python3.9/site-packages/transformers/modeling_utils.py", line 3834, in from_pretrained
    ) = cls._load_pretrained_model(
  File "/home/zx22/.conda/envs/llama/lib/python3.9/site-packages/transformers/modeling_utils.py", line 4294, in _load_pretrained_model
    new_error_msgs, offload_index, state_dict_index = _load_state_dict_into_meta_model(
  File "/home/zx22/.conda/envs/llama/lib/python3.9/site-packages/transformers/modeling_utils.py", line 895, in _load_state_dict_into_meta_model
    set_module_tensor_to_device(model, param_name, param_device, **set_module_kwargs)
  File "/home/zx22/.conda/envs/llama/lib/python3.9/site-packages/accelerate/utils/modeling.py", line 303, in set_module_tensor_to_device
    new_value = value.to(device)
torch.cuda.OutOfMemoryError: CUDA out of memory. Tried to allocate 112.00 MiB. GPU 0 has a total capacty of 39.39 GiB of which 69.00 MiB is free. Process 2719721 has 14.25 GiB memory in use. Process 113328 has 13.27 GiB memory in use. Including non-PyTorch memory, this process has 11.79 GiB memory in use. Of the allocated memory 11.28 GiB is allocated by PyTorch, and 105.59 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
