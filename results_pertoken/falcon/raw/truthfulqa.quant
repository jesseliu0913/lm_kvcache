nohup: ignoring input
2024-06-26:10:34:17,898 INFO     [__main__.py:223] Verbosity set to INFO
2024-06-26:10:34:17,898 INFO     [__init__.py:369] lm_eval.tasks.initialize_tasks() is deprecated and no longer necessary. It will be removed in v0.4.2 release. TaskManager will instead be used.
2024-06-26:10:34:22,423 INFO     [__main__.py:307] Selected Tasks: ['truthfulqa']
2024-06-26:10:34:22,423 INFO     [__main__.py:308] Loading selected tasks...
2024-06-26:10:34:22,424 INFO     [evaluator.py:135] Setting random seed to 0 | Setting numpy seed to 1234 | Setting torch manual seed to 1234
2024-06-26:10:34:22,531 WARNING  [other.py:349] Detected kernel version 5.4.0, which is below the recommended minimum of 5.5.0; this can cause the process to hang. It is recommended to upgrade the kernel to the minimum version or higher.
2024-06-26:10:34:22,531 INFO     [huggingface.py:162] Using device 'cuda'
2024-06-26:10:34:22,764 WARNING  [logging.py:328] 
WARNING: You are currently loading Falcon using legacy code contained in the model repository. Falcon has now been fully ported into the Hugging Face transformers library. For the most up-to-date and high-performance version of the Falcon model code, please update to the latest version of transformers and then load the model without the trust_remote_code=True argument.

Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]/scratch0/zx22/zijie/miniconda3/envs/llama/lib/python3.10/site-packages/torch/_utils.py:831: UserWarning: TypedStorage is deprecated. It will be removed in the future and UntypedStorage will be the only storage class. This should only matter to you if you are using storages directly.  To access UntypedStorage directly, use tensor.untyped_storage() instead of tensor.storage()
  return self.fget.__get__(instance, owner)()
Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]
Traceback (most recent call last):
  File "/scratch0/zx22/zijie/miniconda3/envs/llama/bin/lm_eval", line 8, in <module>
    sys.exit(cli_evaluate())
  File "/scratch1/zx22/zijie/lm-evaluation-harness/lm_eval/__main__.py", line 314, in cli_evaluate
    results = evaluator.simple_evaluate(
  File "/scratch1/zx22/zijie/lm-evaluation-harness/lm_eval/utils.py", line 288, in _wrapper
    return fn(*args, **kwargs)
  File "/scratch1/zx22/zijie/lm-evaluation-harness/lm_eval/evaluator.py", line 166, in simple_evaluate
    lm = lm_eval.api.registry.get_model(model).create_from_arg_string(
  File "/scratch1/zx22/zijie/lm-evaluation-harness/lm_eval/api/model.py", line 134, in create_from_arg_string
    return cls(**args, **args2)
  File "/scratch1/zx22/zijie/lm-evaluation-harness/lm_eval/models/huggingface.py", line 201, in __init__
    self._create_model(
  File "/scratch1/zx22/zijie/lm-evaluation-harness/lm_eval/models/huggingface.py", line 524, in _create_model
    self._model = self.AUTO_MODEL_CLASS.from_pretrained(
  File "/scratch0/zx22/zijie/miniconda3/envs/llama/lib/python3.10/site-packages/transformers/models/auto/auto_factory.py", line 559, in from_pretrained
    return model_class.from_pretrained(
  File "/scratch0/zx22/zijie/miniconda3/envs/llama/lib/python3.10/site-packages/transformers/modeling_utils.py", line 3834, in from_pretrained
    ) = cls._load_pretrained_model(
  File "/scratch0/zx22/zijie/miniconda3/envs/llama/lib/python3.10/site-packages/transformers/modeling_utils.py", line 4294, in _load_pretrained_model
    new_error_msgs, offload_index, state_dict_index = _load_state_dict_into_meta_model(
  File "/scratch0/zx22/zijie/miniconda3/envs/llama/lib/python3.10/site-packages/transformers/modeling_utils.py", line 895, in _load_state_dict_into_meta_model
    set_module_tensor_to_device(model, param_name, param_device, **set_module_kwargs)
  File "/scratch0/zx22/zijie/miniconda3/envs/llama/lib/python3.10/site-packages/accelerate/utils/modeling.py", line 400, in set_module_tensor_to_device
    new_value = value.to(device)
RuntimeError: CUDA error: uncorrectable ECC error encountered
CUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.
For debugging consider passing CUDA_LAUNCH_BLOCKING=1.
Compile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.

