nohup: ignoring input
2024-06-20 00:08:11.756579: I tensorflow/core/platform/cpu_feature_guard.cc:182] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.
To enable the following instructions: AVX2 FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.
2024-06-20 00:08:12.612745: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT
[2024-06-20 00:08:15,312] [INFO] [real_accelerator.py:110:get_accelerator] Setting ds_accelerator to cuda (auto detect)
2024-06-20:00:08:16,761 INFO     [__main__.py:223] Verbosity set to INFO
2024-06-20:00:08:16,761 INFO     [__init__.py:369] lm_eval.tasks.initialize_tasks() is deprecated and no longer necessary. It will be removed in v0.4.2 release. TaskManager will instead be used.
2024-06-20:00:08:21,699 WARNING  [__main__.py:235]  --limit SHOULD ONLY BE USED FOR TESTING.REAL METRICS SHOULD NOT BE COMPUTED USING LIMIT.
2024-06-20:00:08:21,700 INFO     [__main__.py:307] Selected Tasks: ['coqa']
2024-06-20:00:08:21,700 INFO     [__main__.py:308] Loading selected tasks...
2024-06-20:00:08:21,701 INFO     [evaluator.py:135] Setting random seed to 0 | Setting numpy seed to 1234 | Setting torch manual seed to 1234
2024-06-20:00:08:21,707 INFO     [llama_quant.py:163] Using device 'cuda'
The argument `trust_remote_code` is to be used with Auto classes. It has no effect here and is ignored.
Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]Loading checkpoint shards:  50%|█████     | 1/2 [00:03<00:03,  3.00s/it]Loading checkpoint shards: 100%|██████████| 2/2 [00:04<00:00,  1.90s/it]Loading checkpoint shards: 100%|██████████| 2/2 [00:04<00:00,  2.07s/it]
2024-06-20:00:09:00,291 INFO     [evaluator.py:193] get_task_dict has been updated to accept an optional argument, `task_manager`Read more here:https://github.com/EleutherAI/lm-evaluation-harness/blob/main/docs/interface.md#external-library-usage
2024-06-20:00:09:03,281 INFO     [task.py:386] Building contexts for coqa on rank 0...
  0%|          | 0/50 [00:00<?, ?it/s]100%|██████████| 50/50 [00:00<00:00, 60349.70it/s]
2024-06-20:00:09:03,299 INFO     [evaluator.py:365] Running generate_until requests
Running generate_until requests:   0%|          | 0/50 [00:00<?, ?it/s]/home/zx22/.conda/envs/llama/lib/python3.9/site-packages/transformers/generation/configuration_utils.py:492: UserWarning: `do_sample` is set to `False`. However, `temperature` is set to `0.6` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `temperature`.
  warnings.warn(
/home/zx22/.conda/envs/llama/lib/python3.9/site-packages/transformers/generation/configuration_utils.py:497: UserWarning: `do_sample` is set to `False`. However, `top_p` is set to `0.9` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `top_p`.
  warnings.warn(
Running generate_until requests:   2%|▏         | 1/50 [02:27<2:00:11, 147.17s/it]Running generate_until requests:   6%|▌         | 3/50 [04:45<1:09:59, 89.35s/it] Running generate_until requests:  10%|█         | 5/50 [06:58<58:09, 77.55s/it]  Running generate_until requests:  14%|█▍        | 7/50 [09:12<52:20, 73.04s/it]Running generate_until requests:  18%|█▊        | 9/50 [11:25<48:20, 70.74s/it]Running generate_until requests:  22%|██▏       | 11/50 [13:41<45:18, 69.69s/it]Running generate_until requests:  26%|██▌       | 13/50 [15:57<42:36, 69.09s/it]Running generate_until requests:  30%|███       | 15/50 [18:12<40:04, 68.69s/it]Running generate_until requests:  34%|███▍      | 17/50 [20:26<37:24, 68.03s/it]Running generate_until requests:  38%|███▊      | 19/50 [22:38<34:53, 67.54s/it]Running generate_until requests:  42%|████▏     | 21/50 [24:51<32:28, 67.20s/it]Running generate_until requests:  46%|████▌     | 23/50 [27:05<30:11, 67.08s/it]Running generate_until requests:  50%|█████     | 25/50 [29:21<28:04, 67.37s/it]Running generate_until requests:  54%|█████▍    | 27/50 [31:35<25:48, 67.32s/it]Running generate_until requests:  58%|█████▊    | 29/50 [33:49<23:31, 67.21s/it]Running generate_until requests:  62%|██████▏   | 31/50 [36:11<21:38, 68.37s/it]Running generate_until requests:  66%|██████▌   | 33/50 [38:26<19:17, 68.11s/it]Running generate_until requests:  70%|███████   | 35/50 [40:38<16:51, 67.45s/it]Running generate_until requests:  74%|███████▍  | 37/50 [42:50<14:30, 67.00s/it]Running generate_until requests:  78%|███████▊  | 39/50 [45:02<12:13, 66.68s/it]Running generate_until requests:  82%|████████▏ | 41/50 [47:21<10:07, 67.53s/it]Running generate_until requests:  86%|████████▌ | 43/50 [49:34<07:50, 67.16s/it]Running generate_until requests:  90%|█████████ | 45/50 [51:53<05:39, 67.93s/it]Running generate_until requests:  94%|█████████▍| 47/50 [54:14<03:26, 68.71s/it]Running generate_until requests:  98%|█████████▊| 49/50 [56:28<01:08, 68.17s/it]Running generate_until requests: 100%|██████████| 50/50 [56:28<00:00, 67.77s/it]
Passed argument batch_size = auto. Detecting largest batch size
Determined Largest batch size: 2
qh_llama (pretrained=meta-llama/Llama-2-7b-hf,dtype=float16,low_cpu_mem_usage=True,trust_remote_code=True), gen_kwargs: (None), limit: 0.1, num_fewshot: None, batch_size: auto
|Tasks|Version|Filter|n-shot|Metric|Value |   |Stderr|
|-----|------:|------|------|------|-----:|---|-----:|
|coqa |      3|none  |None  |em    |0.0600|±  |0.0339|
|     |       |none  |None  |f1    |0.2251|±  |0.0432|

