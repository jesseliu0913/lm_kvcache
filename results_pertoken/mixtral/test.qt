nohup: ignoring input
2024-07-03:16:40:15,090 INFO     [__main__.py:223] Verbosity set to INFO
2024-07-03:16:40:15,090 INFO     [__init__.py:369] lm_eval.tasks.initialize_tasks() is deprecated and no longer necessary. It will be removed in v0.4.2 release. TaskManager will instead be used.
2024-07-03:16:40:19,027 WARNING  [__main__.py:235]  --limit SHOULD ONLY BE USED FOR TESTING.REAL METRICS SHOULD NOT BE COMPUTED USING LIMIT.
2024-07-03:16:40:19,029 INFO     [__main__.py:307] Selected Tasks: ['coqa']
2024-07-03:16:40:19,029 INFO     [__main__.py:308] Loading selected tasks...
2024-07-03:16:40:19,030 INFO     [evaluator.py:135] Setting random seed to 0 | Setting numpy seed to 1234 | Setting torch manual seed to 1234
2024-07-03:16:40:19,210 WARNING  [other.py:349] Detected kernel version 5.4.0, which is below the recommended minimum of 5.5.0; this can cause the process to hang. It is recommended to upgrade the kernel to the minimum version or higher.
2024-07-03:16:40:19,210 INFO     [mistral_qt.py:166] Using device 'cuda'
The argument `trust_remote_code` is to be used with Auto classes. It has no effect here and is ignored.
Loading checkpoint shards:   0%|          | 0/3 [00:00<?, ?it/s]Loading checkpoint shards:  33%|███▎      | 1/3 [00:02<00:04,  2.25s/it]Loading checkpoint shards:  67%|██████▋   | 2/3 [00:04<00:02,  2.12s/it]Loading checkpoint shards: 100%|██████████| 3/3 [00:06<00:00,  2.04s/it]Loading checkpoint shards: 100%|██████████| 3/3 [00:06<00:00,  2.08s/it]
2024-07-03:16:40:26,189 INFO     [evaluator.py:193] get_task_dict has been updated to accept an optional argument, `task_manager`Read more here:https://github.com/EleutherAI/lm-evaluation-harness/blob/main/docs/interface.md#external-library-usage
2024-07-03:16:40:28,693 INFO     [task.py:386] Building contexts for coqa on rank 0...
The model parameters are of dtype: torch.float16
  0%|          | 0/5 [00:00<?, ?it/s]100%|██████████| 5/5 [00:00<00:00, 35246.25it/s]
2024-07-03:16:40:28,695 INFO     [evaluator.py:365] Running generate_until requests
Running generate_until requests:   0%|          | 0/5 [00:00<?, ?it/s]Running generate_until requests:  20%|██        | 1/5 [00:03<00:15,  3.87s/it]Running generate_until requests:  40%|████      | 2/5 [00:04<00:05,  1.85s/it]Running generate_until requests:  60%|██████    | 3/5 [00:04<00:02,  1.20s/it]Running generate_until requests:  80%|████████  | 4/5 [00:05<00:00,  1.16it/s]Running generate_until requests: 100%|██████████| 5/5 [00:05<00:00,  1.51it/s]Running generate_until requests: 100%|██████████| 5/5 [00:05<00:00,  1.08s/it]
Passed argument batch_size = auto. Detecting largest batch size
Determined Largest batch size: 1
qt_mistral (pretrained=mistralai/Mistral-7B-v0.3;4,dtype=float16,low_cpu_mem_usage=True,trust_remote_code=True), gen_kwargs: (None), limit: 0.01, num_fewshot: None, batch_size: auto
|Tasks|Version|Filter|n-shot|Metric|Value |   |Stderr|
|-----|------:|------|------|------|-----:|---|-----:|
|coqa |      3|none  |None  |em    |0.6333|±  |0.1856|
|     |       |none  |None  |f1    |0.6333|±  |0.1856|

